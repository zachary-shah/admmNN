{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Junk script so i can learn this problem, ignore mostly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg import block_diag, solve_triangular"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions\n",
    "* $n$ - number of training examples\n",
    "* $d$ - feature dimension\n",
    "* $r$ - rank of $X$\n",
    "* $P$ - total enumerations of hidden neurons / ReLU activations\n",
    "* $P_S$ - number of hidden neurons / samples of $D_i$ matrices\n",
    "\n",
    "Matrices\n",
    "* $X \\in \\mathbb{R}^{n \\times d}$ - training data matrix\n",
    "* $y \\in \\mathbb{R}^{n}$ - training labels\n",
    "\n",
    "* $D_i \\in \\mathbb{R}^{n \\times n}, \\ i = 1,\\dots,P_S$ - diagonal matrices sampling hidden layer activations\n",
    "* $F_i = D_i X \\in \\mathbb{R}^{n \\times d}, \\ i = 1,\\dots,P_S$\n",
    "* $G_i = (2 D_i - I_n) X \\in \\mathbb{R}^{n \\times d}, \\ i = 1,\\dots,P_S$\n",
    " \n",
    "Primal Variables  \n",
    "* $v_i \\in \\mathbb{R}^{d}, \\ i = 1,\\dots,P_S$ - vector weights 1\n",
    "* $w_i \\in \\mathbb{R}^{d}, \\ i = 1,\\dots,P_S$ - vector weights 2\n",
    "\n",
    "Introduced Variables\n",
    "* $u_i \\in \\mathbb{R}^{d}, \\ i = 1,\\dots,P_S; \\quad \\quad u_i = v_i \\ \\forall i$ \n",
    "* $z_i \\in \\mathbb{R}^{d}, \\ i = 1,\\dots,P_S; \\quad \\quad z_i = w_i \\ \\forall i$ \n",
    "\n",
    "Slack Variables\n",
    "* $s_i = G_i v_i \\in \\mathbb{R}^{n}, \\ i = 1,\\dots,P_S$\n",
    "* $t_i = G_i w_i \\in \\mathbb{R}^{n}, \\ i = 1,\\dots,P_S$\n",
    "\n",
    "Composite Vectors and Matrices\n",
    "* $u := \\begin{bmatrix} u_1^T & \\dots & u_{P_S}^T & z_1^T & \\dots & z_{P_S}^T \\end{bmatrix}^T \\in \\mathbb{R}^{2 d P_s}$\n",
    "\n",
    "* $v := \\begin{bmatrix} v_1^T & \\dots & v_{P_S}^T & w_1^T & \\dots & w_{P_S}^T \\end{bmatrix}^T \\in \\mathbb{R}^{2 d P_s}$\n",
    "* $s := \\begin{bmatrix} s_1^T & \\dots & s_{P_S}^T & t_1^T & \\dots & t_{P_S}^T \\end{bmatrix}^T \\in \\mathbb{R}^{2 n P_s}$\n",
    "* $F = \\begin{bmatrix} F_1 & \\dots & F_{P_S} & -F_1 & \\dots & -F_{P_S} \\end{bmatrix} \\in \\mathbb{R}^{n \\times (2 d P_S)}, \\ i = 1,\\dots,P_S$\n",
    "* $G = \\text{blockdiag}(G_1, \\ \\dots, \\ G_{P_S}, \\ G_1, \\ \\dots, \\ G_{P_S}) \\in \\mathbb{R}^{n \\times (2 d P_S)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "### user inputs \n",
    "n,d = 100,5\n",
    "\n",
    "# X - data matrix in n x d\n",
    "X = np.random.randn(n,d)\n",
    "\n",
    "# y are targets\n",
    "y = np.random.randn(n,1)\n",
    "\n",
    "r = np.linalg.matrix_rank(X)\n",
    "\n",
    "# add bias term \n",
    "X = np.hstack([X, np.ones((n,1))])\n",
    "d += 1\n",
    "\n",
    "# number samples of D matrices in convex network\n",
    "P_S = 10\n",
    "\n",
    "# number of hidden layers in network \n",
    "m = 3\n",
    "\n",
    "assert m <= P_S\n",
    "\n",
    "# parameters (TODO: parameterize)\n",
    "rho = 1e-5\n",
    "step = 1e-5\n",
    "beta = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: figure out better way to sample D matrices\n",
    "\n",
    "h = np.random.randn(d, P_S)\n",
    "\n",
    "# TODO: assert that no duplicate columns\n",
    "# n x P_S matrix, where each column is the diagonal entries for one D matrix\n",
    "d_diags = X @ h >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# F here is n x (2d*P_S) like paper\n",
    "F = np.hstack([np.hstack([np.diag(d_diags[:,i].astype('uint8')) @ X for i in range(P_S)]),\n",
    "    np.hstack([-np.diag(d_diags[:,i].astype('uint8')) @ X for i in range(P_S)[::-1]])])\n",
    "\n",
    "# G is block diagonal 2*n*P_S x 2*d*P_s\n",
    "Glist = [(2 * np.diag(d_diags[:,i].astype('uint8')) - np.eye(n)) @ X for i in range(P_S)]\n",
    "G = block_diag(*Glist * 2)\n",
    "\n",
    "# weights\n",
    "# u contains u1 ... uP, z1... zP in one long vector\n",
    "u = np.zeros((2 * d * P_S, 1))\n",
    "# v contrains v1 ... vP, w1 ... wP in one long vector\n",
    "v = np.zeros((2 * d * P_S, 1))\n",
    "\n",
    "# slacks s1 ... sP, t1 ... tP\n",
    "s = np.zeros((2 * P_S * n, 1))\n",
    "for i in range(2 * P_S):\n",
    "    # s_i = G_i v_i\n",
    "    s[i*n:(i+1)*n] = Glist[i % P_S] @ v[i*d:(i+1)*d]\n",
    "\n",
    "# dual variables\n",
    "# lam contains lam11 lam12 ... lam1P lam21 lam22 ... lam2P\n",
    "lam = np.zeros((2 * d * P_S,1))\n",
    "# nu contains nu11 nu12 ... nu1P nu21 nu22 ... nu2P\n",
    "nu = np.zeros((2 * n * P_S,1))\n",
    "\n",
    "# precomputes: A in 2dP_s x 2dP_s\n",
    "A = np.eye(2*d*P_S) + F.T @ F / rho + G.T @ G\n",
    "# cholesky factorization\n",
    "L = LA.cholesky(A)\n",
    "# extra precompute for u update step\n",
    "b_1 = F.T @ y / rho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# updates \n",
    "\n",
    "# first, conduct the primal update on u (u1...uP, z1...zP)\n",
    "b = b_1 + v - lam + G.T @ (s - nu)\n",
    "bhat = solve_triangular(L, b, lower=True)\n",
    "u = solve_triangular(L.T, bhat, lower=False)\n",
    "\n",
    "# second, perform updates of v and s\n",
    "# TODO: parallelize v and s updates\n",
    "\n",
    "# upates on v, (v1...vP, w1...wP)\n",
    "for i in range(2 * P_S):\n",
    "    inds = np.arange(d*i, d*(i+1))\n",
    "    v[inds] = np.maximum(1 - beta/(rho * LA.norm(u[inds] + lam[inds])), 0) * (u[inds] + lam[inds])\n",
    "\n",
    "# updates on s\n",
    "for i in range(P_S):\n",
    "    # update s_i\n",
    "    s[i*n:(i+1)*n] = np.maximum(Glist[i] @ u[i*d:(i+1)*d] + nu[i*n:(i+1)*n], 0)\n",
    "    # update t_i\n",
    "    s[(i+P_S)*n:(i+P_S+1)*n] = np.maximum(Glist[i] @ u[(i+P_S)*d:(i+P_S+1)*d] + nu[(i+P_S)*n:(i+P_S+1)*n], 0)\n",
    "\n",
    "# dual updates last\n",
    "lam += lam + step / rho * (u - v)\n",
    "nu += nu + step / rho * (G @ u - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network guranteed not optimal.\n",
      "Network has 0 nonzero neurons.\n"
     ]
    }
   ],
   "source": [
    "# recover u1.... u_ms and alpha1 ... alpha_ms\n",
    "u_star = np.zeros((m, d))\n",
    "alpha_star = np.zeros((m,1))\n",
    "\n",
    "v_star = v.reshape((P_S*2, d))[:P_S]\n",
    "w_star = v.reshape((P_S*2, d))[P_S:P_S*2]\n",
    "\n",
    "# critical number of neurons \n",
    "mstar = np.sum(~ np.isclose(LA.norm(v_star, axis=1), 0)) + np.sum(~ np.isclose(LA.norm(w_star, axis=1), 0))\n",
    "\n",
    "if m > mstar:\n",
    "    print(\"Network guranteed not optimal.\")\n",
    "\n",
    "j = 0\n",
    "for i in range(P_S):\n",
    "    if not np.isclose(LA.norm(v_star[i]), 0):\n",
    "        u_star[j] = v_star[i] / np.sqrt(LA.norm(v_star[i]))\n",
    "        alpha_star[j] = np.sqrt(LA.norm(v_star[i]))\n",
    "        j += 1\n",
    "    if j == m: break\n",
    "for i in range(P_S):\n",
    "    if not np.isclose(LA.norm(w_star[i]), 0):\n",
    "        u_star[j] = w_star[i] / np.sqrt(LA.norm(w_star[i]))\n",
    "        alpha_star[j] = - np.sqrt(LA.norm(w_star[i]))\n",
    "        j += 1\n",
    "    if j == m: break\n",
    "\n",
    "print(f\"Network has {j} nonzero neurons.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
