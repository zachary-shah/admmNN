{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from admm_approximate_solver import Approximate_ReLU_ADMM_Solver\n",
    "from relu_utils import squared_loss, classifcation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n train = 1279\n",
      "n test = 320\n"
     ]
    }
   ],
   "source": [
    "# generate toy data trying to fit noisy data to cosine\n",
    "data = pd.read_csv(\"test_data/wineQuality-red.csv\", delimiter=\";\")\n",
    "X = np.array(data)[:,:11]\n",
    "y = np.array(data.quality)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_train, d = X_train.shape\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "print(f\"n train = {n_train}\")\n",
    "print(f\"n test = {n_test}\")\n",
    "\n",
    "m = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=11, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=10, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TODO: solve simple pytorch problem\n",
    "\n",
    "MLP = nn.Sequential(nn.Linear(d, m),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(m, np.max(y)+1))\n",
    "\n",
    "print(MLP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solver doesn't converge well... Things needing fixing right now: \n",
    "\n",
    "- How to choose m, P_S?\n",
    "- How to choose parameters $\\rho$, step=$\\gamma_{\\alpha}$, $\\beta$? \n",
    "- Converting from optimal weights $v, w$ to $u, \\alpha$. Currently loss during training is consistently high but gets lower when calling .predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss = 20642.500000000004, acc = 0.0\n",
      "iter = 1, loss = 241.66529567373126, acc = 0.6317435496481626\n",
      "iter = 2, loss = 241.4982586121236, acc = 0.6309616888193902\n",
      "iter = 3, loss = 241.36039044967305, acc = 0.6309616888193902\n",
      "iter = 4, loss = 241.2805228628327, acc = 0.6309616888193902\n",
      "iter = 5, loss = 241.23780168681904, acc = 0.6293979671618452\n",
      "iter = 6, loss = 241.2144015295555, acc = 0.6301798279906177\n",
      "iter = 7, loss = 241.20166820306642, acc = 0.6325254104769351\n",
      "iter = 8, loss = 241.1948170469304, acc = 0.6333072713057076\n",
      "iter = 9, loss = 241.19128340761014, acc = 0.6333072713057076\n",
      "iter = 10, loss = 241.1897577444754, acc = 0.63408913213448\n",
      "iter = 11, loss = 241.1895061919421, acc = 0.63408913213448\n",
      "iter = 12, loss = 241.18994266970515, acc = 0.6325254104769351\n",
      "iter = 13, loss = 241.19079382872096, acc = 0.6325254104769351\n",
      "iter = 14, loss = 241.19208672846625, acc = 0.6325254104769351\n",
      "iter = 15, loss = 241.19410465684916, acc = 0.6325254104769351\n",
      "iter = 16, loss = 241.19654947927907, acc = 0.6325254104769351\n",
      "iter = 17, loss = 241.19926734703574, acc = 0.6325254104769351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zachary/Desktop/Stanford/_Spr2023/EE 364B/Project/baADMM/admm_approximate_solver.py:146: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  v[inds] = np.maximum(1 - self.beta/(self.rho * LA.norm(u[inds] + lam[inds])), 0) * (u[inds] + lam[inds])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 18, loss = 241.20215720335898, acc = 0.6325254104769351\n",
      "iter = 19, loss = 241.20522733934948, acc = 0.6333072713057076\n",
      "iter = 20, loss = 241.20846536022057, acc = 0.6325254104769351\n",
      "iter = 21, loss = 241.2118930057887, acc = 0.6325254104769351\n",
      "iter = 22, loss = 241.2155004269168, acc = 0.6325254104769351\n",
      "iter = 23, loss = 241.21944978968895, acc = 0.6325254104769351\n",
      "iter = 24, loss = 241.22365507697816, acc = 0.6325254104769351\n",
      "iter = 25, loss = 241.22813634227836, acc = 0.6325254104769351\n",
      "iter = 26, loss = 241.2327291244362, acc = 0.6325254104769351\n",
      "iter = 27, loss = 241.2373226776444, acc = 0.6325254104769351\n",
      "iter = 28, loss = 241.24210967529524, acc = 0.6325254104769351\n",
      "iter = 29, loss = 241.24693127772593, acc = 0.6325254104769351\n",
      "iter = 30, loss = 241.251828972139, acc = 0.6325254104769351\n",
      "iter = 31, loss = 241.25680100260658, acc = 0.6317435496481626\n",
      "iter = 32, loss = 241.26190161092327, acc = 0.6317435496481626\n",
      "iter = 33, loss = 241.26698686778184, acc = 0.6317435496481626\n",
      "iter = 34, loss = 241.27220802336788, acc = 0.6317435496481626\n",
      "iter = 35, loss = 241.27730241548738, acc = 0.6317435496481626\n",
      "iter = 36, loss = 241.2823213024942, acc = 0.6317435496481626\n",
      "iter = 37, loss = 241.28724852419913, acc = 0.6317435496481626\n",
      "iter = 38, loss = 241.2921376714408, acc = 0.6317435496481626\n",
      "iter = 39, loss = 241.29729587777302, acc = 0.6317435496481626\n",
      "iter = 40, loss = 241.30277857437554, acc = 0.6317435496481626\n",
      "iter = 41, loss = 241.30840357003498, acc = 0.6317435496481626\n",
      "iter = 42, loss = 241.31407967379312, acc = 0.6317435496481626\n",
      "iter = 43, loss = 241.31970952571098, acc = 0.6317435496481626\n",
      "iter = 44, loss = 241.3252747002096, acc = 0.6317435496481626\n",
      "iter = 45, loss = 241.33083083195, acc = 0.6317435496481626\n",
      "iter = 46, loss = 241.3364725414547, acc = 0.6317435496481626\n",
      "iter = 47, loss = 241.34210157670086, acc = 0.6317435496481626\n",
      "iter = 48, loss = 241.3476536403742, acc = 0.6317435496481626\n",
      "iter = 49, loss = 241.35322027421694, acc = 0.6317435496481626\n",
      "iter = 50, loss = 241.358783630667, acc = 0.6317435496481626\n",
      "iter = 51, loss = 241.36441925942395, acc = 0.6317435496481626\n",
      "iter = 52, loss = 241.37009847307309, acc = 0.6317435496481626\n",
      "iter = 53, loss = 241.37575267663706, acc = 0.6309616888193902\n",
      "iter = 54, loss = 241.3814626246954, acc = 0.6309616888193902\n",
      "iter = 55, loss = 241.3871936825677, acc = 0.6309616888193902\n",
      "iter = 56, loss = 241.3930034998957, acc = 0.6309616888193902\n",
      "iter = 57, loss = 241.3987587750616, acc = 0.6309616888193902\n",
      "iter = 58, loss = 241.4044490540402, acc = 0.6309616888193902\n",
      "iter = 59, loss = 241.41018408625325, acc = 0.6309616888193902\n",
      "iter = 60, loss = 241.4159794474338, acc = 0.6309616888193902\n",
      "iter = 61, loss = 241.4216918247238, acc = 0.6309616888193902\n",
      "iter = 62, loss = 241.42739535716728, acc = 0.6309616888193902\n",
      "iter = 63, loss = 241.43312590965644, acc = 0.6317435496481626\n",
      "iter = 64, loss = 241.4388094780461, acc = 0.6317435496481626\n",
      "iter = 65, loss = 241.44447113555015, acc = 0.6317435496481626\n",
      "iter = 66, loss = 241.45012496644878, acc = 0.6317435496481626\n",
      "iter = 67, loss = 241.45577671008883, acc = 0.6317435496481626\n",
      "iter = 68, loss = 241.46156208505508, acc = 0.6317435496481626\n",
      "iter = 69, loss = 241.46728977536554, acc = 0.6317435496481626\n",
      "iter = 70, loss = 241.47293020110078, acc = 0.6317435496481626\n",
      "iter = 71, loss = 241.47850402731174, acc = 0.6317435496481626\n",
      "iter = 72, loss = 241.48412337581772, acc = 0.6317435496481626\n",
      "iter = 73, loss = 241.4897270148733, acc = 0.6317435496481626\n",
      "iter = 74, loss = 241.495407675717, acc = 0.6317435496481626\n",
      "iter = 75, loss = 241.50105505880663, acc = 0.6317435496481626\n",
      "iter = 76, loss = 241.50677782669482, acc = 0.6317435496481626\n",
      "iter = 77, loss = 241.51247524718445, acc = 0.6317435496481626\n",
      "iter = 78, loss = 241.51808952856527, acc = 0.6317435496481626\n",
      "iter = 79, loss = 241.5236869026723, acc = 0.6317435496481626\n",
      "iter = 80, loss = 241.52933991479455, acc = 0.6317435496481626\n",
      "iter = 81, loss = 241.5349814070698, acc = 0.6317435496481626\n",
      "iter = 82, loss = 241.54055957317544, acc = 0.6317435496481626\n",
      "iter = 83, loss = 241.54614091978544, acc = 0.6317435496481626\n",
      "iter = 84, loss = 241.55165271203234, acc = 0.6317435496481626\n",
      "iter = 85, loss = 241.55710242326035, acc = 0.6317435496481626\n",
      "iter = 86, loss = 241.5625383991719, acc = 0.6317435496481626\n",
      "iter = 87, loss = 241.5678722066273, acc = 0.6317435496481626\n",
      "iter = 88, loss = 241.573141865556, acc = 0.6317435496481626\n",
      "iter = 89, loss = 241.57834016232616, acc = 0.6317435496481626\n",
      "iter = 90, loss = 241.58350874729544, acc = 0.6317435496481626\n",
      "iter = 91, loss = 241.588694509631, acc = 0.6317435496481626\n",
      "iter = 92, loss = 241.59384897801615, acc = 0.6317435496481626\n",
      "iter = 93, loss = 241.59898730613168, acc = 0.6317435496481626\n",
      "iter = 94, loss = 241.6041217492651, acc = 0.6317435496481626\n",
      "iter = 95, loss = 241.60924146660116, acc = 0.6317435496481626\n",
      "iter = 96, loss = 241.61429626493197, acc = 0.6317435496481626\n",
      "iter = 97, loss = 241.6193848116443, acc = 0.6317435496481626\n",
      "iter = 98, loss = 241.6244502756597, acc = 0.6317435496481626\n",
      "iter = 99, loss = 241.6295098161363, acc = 0.6317435496481626\n",
      "Network of width 10 has 10 nonzero neurons for non-convex weights.\n",
      "CONVEX PROBLEM WEIGHTS:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 320 is different from 1279)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCONVEX PROBLEM WEIGHTS:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m y_hat_train \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39mpredict(X_train, weights\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC-ReLU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m y_hat_test \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39;49mpredict(X_test, weights\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC-ReLU\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain loss: \u001b[39m\u001b[39m{\u001b[39;00msquared_loss(y_hat_train,\u001b[39m \u001b[39my_train)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain accuracy: \u001b[39m\u001b[39m{\u001b[39;00mclassifcation_accuracy(y_hat_train,\u001b[39m \u001b[39my_train)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Stanford/_Spr2023/EE 364B/Project/baADMM/admm_approximate_solver.py:196\u001b[0m, in \u001b[0;36mApproximate_ReLU_ADMM_Solver.predict\u001b[0;34m(self, X, weights)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39melif\u001b[39;00m weights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC-ReLU\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[1;32m    195\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mP_S):\n\u001b[0;32m--> 196\u001b[0m         y_hat \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdiag(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md_diags[:,i]) \u001b[39m@\u001b[39;49m X \u001b[39m@\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv[i][:,\u001b[39mNone\u001b[39;00m] \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw[i][:,\u001b[39mNone\u001b[39;00m])\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 320 is different from 1279)"
     ]
    }
   ],
   "source": [
    "# solve cvx problem\n",
    "\n",
    "solver = Approximate_ReLU_ADMM_Solver(m=10,\n",
    "                                      P_S=10,\n",
    "                                      rho=0.0001,\n",
    "                                      step=0.0001,\n",
    "                                      beta=0.0001,\n",
    "                                      bias=True,\n",
    "                                      loss_func=squared_loss,\n",
    "                                      acc_func=classifcation_accuracy)\n",
    "\n",
    "solver.optimize(X_train, y_train, max_iter=100, verbose=True)\n",
    "\n",
    "\n",
    "print(\"CONVEX PROBLEM WEIGHTS:\")\n",
    "y_hat_train = solver.predict(X_train, weights=\"C-ReLU\")\n",
    "y_hat_test = solver.predict(X_test, weights=\"C-ReLU\")\n",
    "print(f\"Train loss: {squared_loss(y_hat_train, y_train)}\")\n",
    "print(f\"Train accuracy: {classifcation_accuracy(y_hat_train, y_train)}\")\n",
    "print(f\"Test loss: {squared_loss(y_hat_test, y_test)}\")\n",
    "print(f\"Test accuracy: {classifcation_accuracy(y_hat_test, y_test)}\")\n",
    "\n",
    "print(\"NONCONVEX PROBLEM WEIGHTS:\")\n",
    "y_hat_train = solver.predict(X_train, weights=\"NC-ReLU\")\n",
    "y_hat_test = solver.predict(X_test, weights=\"NC-ReLU\")\n",
    "print(f\"Train loss: {squared_loss(y_hat_train, y_train)}\")\n",
    "print(f\"Train accuracy: {classifcation_accuracy(y_hat_train, y_train)}\")\n",
    "print(f\"Test loss: {squared_loss(y_hat_test, y_test)}\")\n",
    "print(f\"Test accuracy: {classifcation_accuracy(y_hat_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
